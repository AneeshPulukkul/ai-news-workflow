# ğŸ¤– Agentic News Workflow System

An intelligent, automated news aggregation and content generation system that scrapes the latest technology and leadership news, generates high-quality articles and social media posts using AI, and provides a streamlined review interface for content approval.

## ğŸŒŸ Features

### ğŸ“° Intelligent News Scraping
- **Multi-source aggregation**: RSS feeds, News API, and direct web scraping
- **Smart categorization**: Automatic classification of technology and leadership content
- **Reliable sources**: TechCrunch, Ars Technica, The Verge, Harvard Business Review, and more
- **Duplicate detection**: Prevents redundant content collection

### ğŸ§  AI-Powered Content Generation
- **Article creation**: Generates comprehensive 800+ word articles from multiple news sources
- **Social media posts**: Creates platform-optimized content for Twitter and LinkedIn
- **Content summarization**: Intelligent extraction of key insights and trends
- **Customizable output**: Configurable word counts and content styles

### ğŸ“‹ Review & Approval Workflow
- **Web-based interface**: Clean, responsive design for content review
- **Approval system**: Easy approve/reject workflow with feedback options
- **Content editing**: In-browser editing capabilities for fine-tuning
- **Real-time statistics**: Dashboard showing content generation metrics

### ğŸ”„ Automated Scheduling
- **Daily workflows**: Automated scraping and content generation
- **Flexible scheduling**: Configurable timing and frequency
- **Error handling**: Robust error recovery and logging
- **Status monitoring**: Real-time system health checks

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   News Sources  â”‚â”€â”€â”€â–¶â”‚  Scraping Engine â”‚â”€â”€â”€â–¶â”‚   News Database â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ RSS Feeds     â”‚    â”‚ â€¢ RSS Parser     â”‚    â”‚ â€¢ SQLite        â”‚
â”‚ â€¢ News APIs     â”‚    â”‚ â€¢ Web Scraper    â”‚    â”‚ â€¢ Article Store â”‚
â”‚ â€¢ Direct Web    â”‚    â”‚ â€¢ Content Filter â”‚    â”‚ â€¢ Metadata      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Review Interfaceâ”‚â—€â”€â”€â”€â”‚  Content Generatorâ”‚â—€â”€â”€â”€â”‚  AI Processing  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Web Dashboard â”‚    â”‚ â€¢ Article Writer â”‚    â”‚ â€¢ OpenAI GPT    â”‚
â”‚ â€¢ Approval Flow â”‚    â”‚ â€¢ Post Creator   â”‚    â”‚ â€¢ LangChain     â”‚
â”‚ â€¢ Content Edit  â”‚    â”‚ â€¢ Quality Check  â”‚    â”‚ â€¢ Summarization â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- OpenAI API key
- Internet connection

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd agentic_news_workflow
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your OpenAI API key
```

4. **Run the system**
```bash
# Start the review interface
cd review_interface
source venv/bin/activate
python src/main.py

# In another terminal, run the workflow
cd ..
python run_workflow.py --mode full
```

5. **Access the interface**
Open your browser to `http://localhost:5000`

## ğŸ“– Usage Guide

### Command Line Interface

The system provides a flexible CLI for different operations:

```bash
# Run complete daily workflow
python run_workflow.py --mode full

# Run only news scraping
python run_workflow.py --mode scrape

# Generate content for specific category
python run_workflow.py --mode generate --category technology

# Check system status
python run_workflow.py --mode status

# Enable verbose logging
python run_workflow.py --mode full --verbose
```

### Web Interface

1. **Dashboard**: View statistics and system status
2. **Scrape News**: Manually trigger news collection
3. **Generate Content**: Create articles and posts from scraped news
4. **Review Content**: Approve, reject, or edit generated content
5. **Monitor Progress**: Track daily content generation metrics

### Automated Scheduling

Set up automated daily workflows using cron:

```bash
# Daily scraping at 6 AM
0 6 * * * cd /path/to/project && python run_workflow.py --mode scrape

# Daily content generation at 8 AM  
0 8 * * * cd /path/to/project && python run_workflow.py --mode generate
```

## âš™ï¸ Configuration

### News Sources

Configure news sources in `config/config.py`:

```python
NEWS_SOURCES = {
    'technology': [
        'https://techcrunch.com/feed/',
        'https://feeds.arstechnica.com/arstechnica/index',
        'https://www.theverge.com/rss/index.xml'
    ],
    'leadership': [
        'https://feeds.hbr.org/harvardbusiness',
        'https://sloanreview.mit.edu/feed/'
    ]
}
```

### Content Generation

Customize content parameters:

```python
CONTENT_CONFIG = {
    'max_articles_per_day': 5,
    'article_word_count': 800,
    'social_platforms': ['twitter', 'linkedin'],
    'ai_model': 'gpt-4.1-mini'
}
```

### API Configuration

Set up your API keys in `.env`:

```bash
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE=https://api.manus.im/api/llm-proxy/v1
NEWS_API_KEY=your_news_api_key_here  # Optional
```

## ğŸ“ Project Structure

```
agentic_news_workflow/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ config.py          # Main configuration
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scrapers/              # News scraping components
â”‚   â”œâ”€â”€ news_scraper.py    # Main scraper classes
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ content_generator/     # AI content generation
â”‚   â”œâ”€â”€ content_generator.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ review_interface/      # Web interface
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.py       # Flask application
â”‚   â”‚   â”œâ”€â”€ routes/       # API endpoints
â”‚   â”‚   â””â”€â”€ static/       # Frontend files
â”‚   â””â”€â”€ venv/             # Virtual environment
â”œâ”€â”€ data/                  # Database files
â”‚   â”œâ”€â”€ news_database.db
â”‚   â””â”€â”€ content_database.db
â”œâ”€â”€ logs/                  # Log files
â”œâ”€â”€ run_workflow.py        # Main workflow runner
â”œâ”€â”€ deployment_guide.md    # Detailed deployment instructions
â””â”€â”€ README.md             # This file
```

## ğŸ”§ API Reference

### Content Management API

- `GET /api/content/pending` - Get pending content for review
- `POST /api/content/articles/{id}/approve` - Approve an article
- `POST /api/content/articles/{id}/reject` - Reject an article
- `POST /api/content/articles/{id}/edit` - Edit an article
- `POST /api/content/posts/{id}/approve` - Approve a social post
- `POST /api/content/generate` - Trigger content generation
- `POST /api/content/scrape` - Trigger news scraping
- `GET /api/content/stats` - Get system statistics

### Workflow API

The `WorkflowRunner` class provides programmatic access:

```python
from run_workflow import WorkflowRunner

runner = WorkflowRunner()

# Run daily workflow
result = runner.run_daily_workflow(['technology'])

# Get system status
status = runner.get_status()

# Run specific components
scraping_result = runner.run_scraping_only()
generation_result = runner.run_generation_only('leadership')
```

## ğŸ› ï¸ Development

### Setting up Development Environment

1. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. **Install development dependencies**
```bash
pip install -r requirements.txt
pip install pytest black flake8  # Development tools
```

3. **Run tests**
```bash
python test_scraper.py
python test_content_generator.py
```

### Adding New News Sources

1. Add RSS feed URL to `config/config.py`
2. Test with `python run_workflow.py --mode scrape`
3. Verify articles appear in database

### Customizing Content Generation

1. Modify prompts in `content_generator/content_generator.py`
2. Adjust parameters in `config/config.py`
3. Test with `python run_workflow.py --mode generate`

## ğŸ“Š Monitoring & Analytics

### Log Files

- `logs/news_workflow.log` - Main application logs
- `review_interface/logs/news_workflow.log` - Web interface logs

### Database Queries

```sql
-- Check recent articles
SELECT title, source, scraped_date FROM articles 
WHERE date(scraped_date) = date('now') 
ORDER BY scraped_date DESC;

-- View content generation stats
SELECT status, COUNT(*) as count 
FROM generated_articles 
GROUP BY status;

-- Monitor daily performance
SELECT date(generated_date) as date, 
       COUNT(*) as articles_generated
FROM generated_articles 
GROUP BY date(generated_date)
ORDER BY date DESC;
```

### Performance Metrics

- **Scraping Rate**: ~2-3 articles per minute
- **Generation Time**: ~30-60 seconds per article
- **API Usage**: ~1000-2000 tokens per article
- **Storage**: ~1MB per 100 articles

## ğŸ”’ Security & Privacy

### Data Protection
- Local SQLite databases (no cloud storage)
- API keys stored in environment variables
- No personal data collection
- Configurable data retention policies

### Access Control
- Web interface runs on localhost by default
- No authentication required for local use
- HTTPS recommended for production deployment
- Rate limiting on API endpoints

## ğŸš¨ Troubleshooting

### Common Issues

**"Module not found" errors**
```bash
pip install -r requirements.txt
source venv/bin/activate  # Ensure virtual environment is active
```

**"Database locked" errors**
```bash
# Check for running processes
ps aux | grep python
# Kill any hanging processes
pkill -f "python.*workflow"
```

**"API rate limit exceeded"**
- Reduce content generation frequency
- Check OpenAI usage dashboard
- Consider upgrading API plan

**"No articles scraped"**
- Check internet connection
- Verify RSS feed URLs are accessible
- Review logs for specific error messages

### Debug Mode

Enable verbose logging for troubleshooting:

```bash
python run_workflow.py --mode full --verbose
```

### Getting Help

1. Check the logs in `logs/news_workflow.log`
2. Review the deployment guide for detailed setup instructions
3. Test individual components using the test scripts
4. Check API key configuration and permissions

## ğŸ¤ Contributing

We welcome contributions! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

### Development Guidelines

- Follow PEP 8 style guidelines
- Add docstrings to all functions
- Include error handling and logging
- Test with multiple news sources
- Update documentation for new features

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- OpenAI for providing the GPT models
- LangChain for the AI framework
- Flask for the web framework
- Beautiful Soup for web scraping capabilities
- All the news sources for providing RSS feeds

---

**Built with â¤ï¸ by the Manus AI Team**

For more information, visit our [documentation](deployment_guide.md) or contact our support team.

